{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsKUC4Gf0vUb159RLWbugt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import kagglehub\n","\n","# Herunterladen des Datensatzes\n","path = kagglehub.dataset_download(\"uciml/breast-cancer-wisconsin-data\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_vX_ldQx2SA","executionInfo":{"status":"ok","timestamp":1755594358258,"user_tz":-120,"elapsed":1854,"user":{"displayName":"Sebastian Steininger","userId":"02820484620863115156"}},"outputId":"73302e36-2a5c-4483-c72f-403de3a3ddab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/breast-cancer-wisconsin-data\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","df = pd.read_csv(path+\"/data.csv\")\n","print(\"\\nDataFrame loaded successfully:\")\n","\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jp2VAafq1hDn","executionInfo":{"status":"ok","timestamp":1755594358351,"user_tz":-120,"elapsed":76,"user":{"displayName":"Sebastian Steininger","userId":"02820484620863115156"}},"outputId":"2596abbb-b10a-40c9-81bc-a56daed63013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame loaded successfully:\n","         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0    842302         M        17.99         10.38          122.80     1001.0   \n","1    842517         M        20.57         17.77          132.90     1326.0   \n","2  84300903         M        19.69         21.25          130.00     1203.0   \n","3  84348301         M        11.42         20.38           77.58      386.1   \n","4  84358402         M        20.29         14.34          135.10     1297.0   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0          0.11840           0.27760          0.3001              0.14710   \n","1          0.08474           0.07864          0.0869              0.07017   \n","2          0.10960           0.15990          0.1974              0.12790   \n","3          0.14250           0.28390          0.2414              0.10520   \n","4          0.10030           0.13280          0.1980              0.10430   \n","\n","   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n","0  ...          17.33           184.60      2019.0            0.1622   \n","1  ...          23.41           158.80      1956.0            0.1238   \n","2  ...          25.53           152.50      1709.0            0.1444   \n","3  ...          26.50            98.87       567.7            0.2098   \n","4  ...          16.67           152.20      1575.0            0.1374   \n","\n","   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n","0             0.6656           0.7119                0.2654          0.4601   \n","1             0.1866           0.2416                0.1860          0.2750   \n","2             0.4245           0.4504                0.2430          0.3613   \n","3             0.8663           0.6869                0.2575          0.6638   \n","4             0.2050           0.4000                0.1625          0.2364   \n","\n","   fractal_dimension_worst  Unnamed: 32  \n","0                  0.11890          NaN  \n","1                  0.08902          NaN  \n","2                  0.08758          NaN  \n","3                  0.17300          NaN  \n","4                  0.07678          NaN  \n","\n","[5 rows x 33 columns]\n"]}]},{"cell_type":"code","source":["class Perceptron:\n","  def __init__(self, weights, bias):\n","    \"\"\" Konstruiert ein neues Perzeptron.\n","\n","    Args:\n","      weights: Die initialen Gewichte des Perzeptrons.\n","      bias: Der initiale Bias des Perzeptrons.\n","    \"\"\"\n","    self.weights = weights\n","    self.bias = bias\n","\n","  def heaviside_step_fn(x):\n","    \"\"\"Setzt die mathematische Heaviside-Schrittfunktion um.\n","\n","    Args:\n","      x: Der Parameter der Funktion.\n","\n","    Returns:\n","      1 wenn x>=0 sonst 0\n","    \"\"\"\n","    return 1 if x >= 0 else 0\n","\n","  def predict(self, input):\n","    \"\"\" Errechnet den Ausgang des Perzeptrons, basierend auf den Eingangswerten,\n","        Gewichten und dem Bias.\n","\n","    Args:\n","      input: Die Eingangswerte des Perzeptrons.\n","\n","    Returns:\n","      Der Ausgabewert (0 oder 1) des Perzeptrons.\n","    \"\"\"\n","    sum = self.linear(input)\n","    return Perceptron.heaviside_step_fn(sum)\n","\n","  def linear(self, input):\n","    \"\"\" Errechnet den gewichteten Durchschnitt der Eingangswerte plus den Bias.\n","\n","    Args:\n","      input: Die Eingangswerte des Perzeptrons.\n","\n","    Returns:\n","      Der gewichtete Durchschnitt aller Werte plus der Bias.\n","    \"\"\"\n","    sum = self.bias\n","    for i in range(len(input)):\n","      sum += input[i] * self.weights[i]\n","    return sum\n","\n","  def train(self, x, y_true, learning_rate = 0.005):\n","    \"\"\" Trainiert das Perzeptron mit den gegebenen Daten.\n","\n","    Args:\n","      x: Die Eingabewerte des Perzeptrons.\n","      y_true: Der gewünschte Ausgabewert des Perzeptrons.\n","      learning_rate: Die Lernrate des Perzeptrons.\n","\n","    Returns:\n","      Den absoluten Fehler des Perzeptrons.\n","    \"\"\"\n","    # Die Vorhersage vor der Bias-/Gewichtsanpassung.\n","    y_hat = self.predict(x)\n","    # Der Fehler durch die Vorhersage.\n","    error = y_true - y_hat\n","\n","    # Anpassen der Gewichte und des Bias basierend auf dem Fehler,\n","    # Lernrate und Eingabewert\n","    self.weights += learning_rate * error * x\n","    self.bias += learning_rate * error\n","    return abs(error)\n","\n","  def __str__(self):\n","    return f\"Gewichte: {self.weights}, Bias: {self.bias}\""],"metadata":{"id":"RCphVTE7HsEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","df.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)\n","\n","df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n","\n","X = df.drop('diagnosis', axis=1)\n","y = df['diagnosis']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","X_train_np = X_train.to_numpy()\n","X_test_np = X_test.to_numpy()\n","y_train_np = y_train.to_numpy()\n","y_test_np = y_test.to_numpy()\n","\n","n_features = X_train_np.shape[1]\n","initial_weights = np.random.rand(n_features)\n","initial_bias = np.random.rand()\n","\n","perceptron = Perceptron(initial_weights, initial_bias)\n","\n","# Definieren der Hyperparameter für das Training\n","EPOCHS = 100\n","LEARNING_RATE = 0.01\n","\n","print(\"\\nStarting Perceptron Training...\")\n","\n","for epoch in range(EPOCHS):\n","    total_error = 0\n","    for x,y in zip(X_train_np, y_train_np):\n","      total_error += perceptron.train(x, y, LEARNING_RATE)\n","\n","    print(f\"Epoch {epoch + 1}, Total Error: {total_error}\")\n","\n","print(\"Perceptron Training Finished.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Xg7YQR5JYGz","executionInfo":{"status":"ok","timestamp":1755594360278,"user_tz":-120,"elapsed":1878,"user":{"displayName":"Sebastian Steininger","userId":"02820484620863115156"}},"outputId":"cdb1b2f9-0bb7-4b00-b831-038aab48176c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting Perceptron Training...\n","Epoch 1, Total Error: 177\n","Epoch 2, Total Error: 141\n","Epoch 3, Total Error: 109\n","Epoch 4, Total Error: 109\n","Epoch 5, Total Error: 96\n","Epoch 6, Total Error: 92\n","Epoch 7, Total Error: 73\n","Epoch 8, Total Error: 94\n","Epoch 9, Total Error: 76\n","Epoch 10, Total Error: 74\n","Epoch 11, Total Error: 71\n","Epoch 12, Total Error: 67\n","Epoch 13, Total Error: 69\n","Epoch 14, Total Error: 68\n","Epoch 15, Total Error: 77\n","Epoch 16, Total Error: 77\n","Epoch 17, Total Error: 59\n","Epoch 18, Total Error: 62\n","Epoch 19, Total Error: 65\n","Epoch 20, Total Error: 73\n","Epoch 21, Total Error: 73\n","Epoch 22, Total Error: 69\n","Epoch 23, Total Error: 62\n","Epoch 24, Total Error: 61\n","Epoch 25, Total Error: 57\n","Epoch 26, Total Error: 65\n","Epoch 27, Total Error: 63\n","Epoch 28, Total Error: 59\n","Epoch 29, Total Error: 57\n","Epoch 30, Total Error: 65\n","Epoch 31, Total Error: 67\n","Epoch 32, Total Error: 64\n","Epoch 33, Total Error: 62\n","Epoch 34, Total Error: 62\n","Epoch 35, Total Error: 62\n","Epoch 36, Total Error: 60\n","Epoch 37, Total Error: 57\n","Epoch 38, Total Error: 57\n","Epoch 39, Total Error: 60\n","Epoch 40, Total Error: 57\n","Epoch 41, Total Error: 69\n","Epoch 42, Total Error: 57\n","Epoch 43, Total Error: 57\n","Epoch 44, Total Error: 70\n","Epoch 45, Total Error: 57\n","Epoch 46, Total Error: 69\n","Epoch 47, Total Error: 57\n","Epoch 48, Total Error: 61\n","Epoch 49, Total Error: 69\n","Epoch 50, Total Error: 57\n","Epoch 51, Total Error: 69\n","Epoch 52, Total Error: 68\n","Epoch 53, Total Error: 61\n","Epoch 54, Total Error: 56\n","Epoch 55, Total Error: 67\n","Epoch 56, Total Error: 71\n","Epoch 57, Total Error: 56\n","Epoch 58, Total Error: 69\n","Epoch 59, Total Error: 68\n","Epoch 60, Total Error: 70\n","Epoch 61, Total Error: 53\n","Epoch 62, Total Error: 57\n","Epoch 63, Total Error: 57\n","Epoch 64, Total Error: 66\n","Epoch 65, Total Error: 57\n","Epoch 66, Total Error: 61\n","Epoch 67, Total Error: 71\n","Epoch 68, Total Error: 56\n","Epoch 69, Total Error: 66\n","Epoch 70, Total Error: 69\n","Epoch 71, Total Error: 64\n","Epoch 72, Total Error: 58\n","Epoch 73, Total Error: 69\n","Epoch 74, Total Error: 61\n","Epoch 75, Total Error: 58\n","Epoch 76, Total Error: 69\n","Epoch 77, Total Error: 61\n","Epoch 78, Total Error: 67\n","Epoch 79, Total Error: 60\n","Epoch 80, Total Error: 63\n","Epoch 81, Total Error: 54\n","Epoch 82, Total Error: 55\n","Epoch 83, Total Error: 68\n","Epoch 84, Total Error: 56\n","Epoch 85, Total Error: 62\n","Epoch 86, Total Error: 57\n","Epoch 87, Total Error: 64\n","Epoch 88, Total Error: 60\n","Epoch 89, Total Error: 57\n","Epoch 90, Total Error: 57\n","Epoch 91, Total Error: 64\n","Epoch 92, Total Error: 62\n","Epoch 93, Total Error: 60\n","Epoch 94, Total Error: 57\n","Epoch 95, Total Error: 64\n","Epoch 96, Total Error: 58\n","Epoch 97, Total Error: 54\n","Epoch 98, Total Error: 55\n","Epoch 99, Total Error: 62\n","Epoch 100, Total Error: 57\n","Perceptron Training Finished.\n"]}]},{"cell_type":"code","source":["print(\"\\nFinal Perceptron:\", perceptron)\n","\n","# Evaluieren der Vorhersagen des Test-Datensatzes.\n","correct_predictions = 0\n","for x,y in zip(X_test_np, y_test_np):\n","    y_pred_i = perceptron.predict(x)\n","    if y_pred_i == y:\n","        correct_predictions += 1\n","\n","accuracy = correct_predictions / X_test_np.shape[0]\n","print(f\"\\nAccuracy on Test Set: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzXeUs7vUHTl","executionInfo":{"status":"ok","timestamp":1755594360315,"user_tz":-120,"elapsed":33,"user":{"displayName":"Sebastian Steininger","userId":"02820484620863115156"}},"outputId":"8d87305d-547a-4f49-e1b2-1123c245e59f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Final Perceptron: Gewichte: [-4.48755551e+01  2.47742343e+01 -2.21880782e+02 -3.18260148e+01\n","  4.69556230e-01  1.52082909e+00  2.85234258e+00  1.81180499e+00\n","  2.58531864e-01  1.61264105e-01 -6.89385775e-02  2.51749671e+00\n","  9.94237090e+00  9.77173208e+01  8.32829830e-02  1.32205970e+00\n","  1.01906494e+00  5.86459016e-01  5.69110198e-01  9.36861550e-01\n"," -4.58613002e+01  6.05224546e+01 -1.61884849e+02  7.43117707e+01\n","  2.29527394e-01  6.45916468e+00  7.63860848e+00  2.62271313e+00\n","  9.45685690e-01  7.91180548e-01], Bias: -6.087944009104582\n","\n","Accuracy on Test Set: 85.96%\n"]}]}]}