{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RNbXb_rpu9Mu",
        "outputId": "d4cd9865-5a6f-47d4-f868-2717448f10c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sätze: ['das schnelle auto fährt die straße entlang', 'ein auto ist ein fahrzeug', 'die katze sitzt auf dem sofa', 'das sofa ist sehr bequem', 'ein sofa und ein tisch stehen im wohnzimmer']\n",
            "\n",
            "--- Auto-Complete Simulation (N=3) ---\n",
            "Nach 'das schnelle' mit Präfix 'a': ['auto']\n",
            "Nach 'ein' mit Präfix 'f': []\n",
            "Nach 'auf dem' mit Präfix 's': ['sofa']\n",
            "Nach 'schnelle katze' mit Präfix 's': []\n",
            "Nach '' (Satzanfang) mit Präfix 'd': ['das', 'die']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Einfache Bereinigung und Tokenisierung.\"\"\"\n",
        "    text = text.lower()\n",
        "    # Nur Buchstaben, Leerzeichen und einige Satzzeichen behalten\n",
        "    text = re.sub(r'[^a-zäöüß\\s.,!?;]', '', text)\n",
        "    return text\n",
        "\n",
        "def sent_tokenize(text):\n",
        "    \"\"\"Teilt Text in Sätze auf.\"\"\"\n",
        "    # Teilen bei Satzzeichen, die auf einen Punkt, Fragezeichen oder Ausrufezeichen folgen\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    # Entferne leere Strings aus der Liste\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "def word_tokenize(sentence):\n",
        "    \"\"\"Teilt Satz in Wörter auf.\"\"\"\n",
        "    words = sentence.split()\n",
        "    return words\n",
        "\n",
        "def build_ngram_counts(sentences, n=3):\n",
        "    \"\"\"Zählt N-Gramm-Häufigkeiten.\"\"\"\n",
        "    ngram_counts = defaultdict(lambda: Counter())\n",
        "    # Füge Start- und End-Symbole hinzu\n",
        "    start_symbol = '<s>'\n",
        "    end_symbol = '</s>'\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Füge Start- und End-Symbole hinzu\n",
        "        words = [start_symbol] * (n - 1) + word_tokenize(sentence) + [end_symbol]\n",
        "        # Generiere N-Gramme und zähle sie\n",
        "        for i in range(len(words) - n + 1):\n",
        "            # Der Kontext ist die Sequenz der ersten n-1 Wörter des N-Gramms\n",
        "            context = tuple(words[i : i + n - 1])\n",
        "            # Das nächste Wort ist das letzte Wort des N-Gramms\n",
        "            next_word = words[i + n - 1]\n",
        "            ngram_counts[context][next_word] += 1\n",
        "\n",
        "    return ngram_counts\n",
        "\n",
        "def predict_next_word_ngram(ngram_counts, context_words, prefix, n=3):\n",
        "    \"\"\"\n",
        "    Sagt die nächsten Wörter basierend auf den Kontextwörtern und einem Präfix voraus.\n",
        "    \"\"\"\n",
        "    # Stelle sicher, dass der Kontext die richtige Länge hat (N-1 Wörter)\n",
        "    # Wenn weniger als N-1 Wörter verfügbar sind, verwenden wir Start-Symbole\n",
        "    start_symbol = '<s>'\n",
        "    if len(context_words) < n - 1:\n",
        "        # Füge so viele Start-Symbole wie nötig am Anfang hinzu\n",
        "        context = tuple([start_symbol] * (n - 1 - len(context_words)) + list(context_words))\n",
        "    else:\n",
        "        # Nimm die letzten n-1 Wörter als Kontext\n",
        "        context = tuple(context_words[-(n - 1):])\n",
        "\n",
        "    suggestions = []\n",
        "    # Finde alle Wörter, die nach diesem Kontext kommen\n",
        "    potential_next_words = ngram_counts.get(context, {})\n",
        "\n",
        "    # Filtere Wörter, die mit dem Präfix beginnen und sortiere nach Häufigkeit\n",
        "    for next_word, count in potential_next_words.items():\n",
        "        # Ignoriere das End-Symbol in den Vorschlägen\n",
        "        if next_word == '</s>':\n",
        "            continue\n",
        "        if next_word.startswith(prefix):\n",
        "            # Berechne die Wahrscheinlichkeit (optional, Zählungen reichen oft für Sortierung)\n",
        "            # total_count = sum(potential_next_words.values())\n",
        "            # probability = count / total_count if total_count > 0 else 0\n",
        "            suggestions.append((next_word, count)) # Speichern als (Wort, Anzahl)\n",
        "\n",
        "    # Sortiere absteigend nach Häufigkeit\n",
        "    suggestions.sort(key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Gib nur die Wörter zurück (z.B. Top 5)\n",
        "    return [word for word, count in suggestions[:5]]\n",
        "\n",
        "# --- Beispielhafte Nutzung ---\n",
        "\n",
        "# Ein etwas größerer Beispielkorpus mit Sätzen\n",
        "corpus = \"\"\"\n",
        "Das schnelle Auto fährt die Straße entlang.\n",
        "Ein Auto ist ein Fahrzeug.\n",
        "Die Katze sitzt auf dem Sofa.\n",
        "Das Sofa ist sehr bequem.\n",
        "Ein Sofa und ein Tisch stehen im Wohnzimmer.\n",
        "\"\"\"\n",
        "\n",
        "# N-Gramm Ordnung festlegen (z.B. 3 für Trigramme)\n",
        "N_ORDER = 3\n",
        "\n",
        "# 1. Korpus vorbereiten und Tokenisieren\n",
        "cleaned_corpus = clean_text(corpus)\n",
        "sentences = sent_tokenize(cleaned_corpus)\n",
        "print(f\"Sätze: {sentences}\\n\")\n",
        "\n",
        "# 2. N-Gramm-Modell erstellen\n",
        "ngram_counts = build_ngram_counts(sentences, n=N_ORDER)\n",
        "# print(f\"{N_ORDER}-Gramm Zählungen: {ngram_counts}\\n\") # Zur Überprüfung\n",
        "\n",
        "# 3. Auto-Complete simulieren\n",
        "print(f\"--- Auto-Complete Simulation (N={N_ORDER}) ---\")\n",
        "\n",
        "# Beispiel 1: Nach \"das schnelle\" und Präfix \"a\"\n",
        "context = [\"das\", \"schnelle\"]\n",
        "prefix = \"a\"\n",
        "vorschlaege = predict_next_word_ngram(ngram_counts, context, prefix, n=N_ORDER)\n",
        "print(f\"Nach '{' '.join(context)}' mit Präfix '{prefix}': {vorschlaege}\")\n",
        "# Erwartung: 'auto'\n",
        "\n",
        "# Beispiel 2: Nach \"ein\" und Präfix \"f\" (Kontext kürzer als N-1, benutzt Start-Symbole)\n",
        "context = [\"ein\"]\n",
        "prefix = \"f\"\n",
        "vorschlaege = predict_next_word_ngram(ngram_counts, context, prefix, n=N_ORDER)\n",
        "print(f\"Nach '{' '.join(context)}' mit Präfix '{prefix}': {vorschlaege}\")\n",
        "# Erwartung: 'fahrzeug'\n",
        "\n",
        "# Beispiel 3: Nach \"auf dem\" und Präfix \"s\"\n",
        "context = [\"auf\", \"dem\"]\n",
        "prefix = \"s\"\n",
        "vorschlaege = predict_next_word_ngram(ngram_counts, context, prefix, n=N_ORDER)\n",
        "print(f\"Nach '{' '.join(context)}' mit Präfix '{prefix}': {vorschlaege}\")\n",
        "# Erwartung: 'sofa'\n",
        "\n",
        "# Beispiel 4: Nach einem Kontext, der nicht im Training vorkam (z.B. \"schnelle katze\")\n",
        "context = [\"schnelle\", \"katze\"] # Diese Sequenz gibt es im Korpus nicht\n",
        "prefix = \"s\"\n",
        "vorschlaege = predict_next_word_ngram(ngram_counts, context, prefix, n=N_ORDER)\n",
        "print(f\"Nach '{' '.join(context)}' mit Präfix '{prefix}': {vorschlaege}\")\n",
        "# Erwartung: Leere Liste oder Fallback auf niedrigere N-Gramme (nicht in dieser einfachen Impl.)\n",
        "\n",
        "# Beispiel 5: Am Satzanfang (Kontext enthält Start-Symbole) mit Präfix \"d\"\n",
        "context = [] # Leerer Kontext bedeutet Satzanfang\n",
        "prefix = \"d\"\n",
        "vorschlaege = predict_next_word_ngram(ngram_counts, context, prefix, n=N_ORDER)\n",
        "print(f\"Nach '{' '.join(context)}' (Satzanfang) mit Präfix '{prefix}': {vorschlaege}\")\n",
        "# Erwartung: 'das'"
      ]
    }
  ]
}