{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt8AgMPvzCkt",
        "outputId": "7faec3df-38c8-4a4a-9b4b-84beff9d2b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "# Kleines Modell: de_core_news_sm\n",
        "# Größeres Modell: de_core_news_md\n",
        "!python -m spacy download de_core_news_sm\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Deutsches Sprachmodell laden\n",
        "nlp = spacy.load(\"de_core_news_sm\")  # Kleines deutsches Modell\n",
        "\n",
        "def build_ngram_model_with_spacy(text, n):\n",
        "    # Text mit SpaCy verarbeiten\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokens extrahieren und filtern\n",
        "    tokens = [token.text.lower() for token in doc\n",
        "              if not token.is_punct and not token.is_space]\n",
        "\n",
        "    # N-Gramm-Zählungen initialisieren\n",
        "    ngram_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Durch alle möglichen N-Gramme iterieren\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        # Die ersten n-1 Wörter sind der Kontext\n",
        "        context = tuple(tokens[i:i+n-1])\n",
        "        next_word = tokens[i+n-1]\n",
        "\n",
        "        # Mit defaultdict brauchen wir keine Existenzprüfung mehr\n",
        "        ngram_counts[context][next_word] += 1\n",
        "\n",
        "    return ngram_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "url = \"https://downloads.wortschatz-leipzig.de/corpora/deu_mixed-typical_2011_10K.tar.gz\"\n",
        "filename = \"deu_mixed-typical_2011_10K.tar.gz\"\n",
        "extracted_dir = \"deu_mixed-typical_2011_10K\"\n",
        "text_file_name = \"deu_mixed-typical_2011_10K-sentences.txt\" # Annahme basierend auf typischen Formaten\n",
        "\n",
        "# 1. Datei herunterladen\n",
        "print(f\"Lade Datei von {url} herunter...\")\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.raw.read())\n",
        "    print(\"Download abgeschlossen.\")\n",
        "else:\n",
        "    print(f\"Fehler beim Herunterladen: Statuscode {response.status_code}\")\n",
        "\n",
        "# 2. Datei entpacken\n",
        "print(f\"Entpacke Datei {filename}...\")\n",
        "if os.path.exists(filename):\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall(\".\")\n",
        "    print(\"Entpacken abgeschlossen.\")\n",
        "else:\n",
        "    print(f\"Fehler: Datei {filename} nicht gefunden.\")\n",
        "\n",
        "# 3. Text extrahieren\n",
        "print(f\"Extrahiere Text aus {text_file_name}...\")\n",
        "full_text = \"\"\n",
        "file_path = os.path.join(extracted_dir, text_file_name)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Zeilenaufbau: Zeilennummer + Tab + Text\n",
        "            parts = line.strip().split('\\t', 1)\n",
        "            if len(parts) > 1:\n",
        "                full_text += parts[1] + \" \" # Füge den Text und ein Leerzeichen hinzu\n",
        "    print(\"Textextraktion abgeschlossen.\")\n",
        "else:\n",
        "    print(f\"Fehler: Textdatei {file_path} nicht gefunden. Bitte prüfen Sie den Dateinamen im Tar-Archiv.\")\n",
        "\n",
        "\n",
        "# 4. N-Gramm-Modell generieren (nutzt die vorhandene Funktion)\n",
        "if full_text:\n",
        "    print(\"Generiere N-Gramm-Modell...\")\n",
        "    # Verwenden Sie ein 3-Gramm-Modell (Kontext von 2 Wörtern)\n",
        "    ngram_model = build_ngram_model_with_spacy(full_text, n=3)\n",
        "    print(\"Modellgenerierung abgeschlossen.\")\n",
        "\n",
        "    # 5. Auto-Complete Funktion\n",
        "    def autocomplete(model, prompt, n=3):\n",
        "        # Verarbeite den Prompt mit SpaCy\n",
        "        doc = nlp(prompt)\n",
        "        # Extrahiere die letzten n-1 Tokens des Prompts\n",
        "        # Stelle sicher, dass wir mindestens n-1 Tokens haben, bevor wir slicen\n",
        "        prompt_tokens = [token.text.lower() for token in doc\n",
        "                         if not token.is_punct and not token.is_space]\n",
        "\n",
        "        # Wenn der Prompt weniger als n-1 Tokens hat, können wir keine Vorhersage treffen\n",
        "        if len(prompt_tokens) < n - 1:\n",
        "            return []\n",
        "\n",
        "        context = tuple(prompt_tokens[-(n-1):]) # Die letzten n-1 Tokens bilden den Kontext\n",
        "\n",
        "        # Überprüfe, ob der Kontext im Modell vorhanden ist\n",
        "        if context in model:\n",
        "            # Hole dir die möglichen nächsten Wörter und ihre Zählungen\n",
        "            next_words_counts = model[context]\n",
        "            # Sortiere die Vorschläge nach Häufigkeit (absteigend)\n",
        "            suggestions = sorted(next_words_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "            # Gib die vorgeschlagenen Wörter zurück\n",
        "            return [word for word, count in suggestions]\n",
        "        else:\n",
        "            # Wenn der Kontext nicht im Modell gefunden wird, gib eine leere Liste zurück\n",
        "            return []\n",
        "\n",
        "    # Beispiel für die Verwendung des Auto-Complete\n",
        "    print(\"\\nAuto-Complete Beispiel:\")\n",
        "    prompt1 = \"Er wurde im\"\n",
        "    suggestions1 = autocomplete(ngram_model, prompt1, n=3)\n",
        "    print(f\"Prompt: '{prompt1}'\")\n",
        "    print(f\"Vorschläge: {suggestions1[:5]}\") # Zeige die Top 5 Vorschläge\n",
        "\n",
        "    prompt2 = \"Es gibt sie\"\n",
        "    suggestions2 = autocomplete(ngram_model, prompt2, n=3)\n",
        "    print(f\"\\nPrompt: '{prompt2}'\")\n",
        "    print(f\"Vorschläge: {suggestions2[:5]}\") # Zeige die Top 5 Vorschläge\n",
        "\n",
        "    prompt3 = \"Dieses Wort kommt nicht\"\n",
        "    suggestions3 = autocomplete(ngram_model, prompt3, n=3)\n",
        "    print(f\"\\nPrompt: '{prompt3}'\")\n",
        "    print(f\"Vorschläge: {suggestions3[:5]}\") # Zeige die Top 5 Vorschläge (sollte leer sein, wenn Kontext unbekannt)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Kein Text zum Modellieren gefunden. Auto-Complete nicht verfügbar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f6rETWsGmHD",
        "outputId": "d6ebcbcf-61b0-4e1c-bacc-3ccad0a8c302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lade Datei von https://downloads.wortschatz-leipzig.de/corpora/deu_mixed-typical_2011_10K.tar.gz herunter...\n",
            "Download abgeschlossen.\n",
            "Entpacke Datei deu_mixed-typical_2011_10K.tar.gz...\n",
            "Entpacken abgeschlossen.\n",
            "Extrahiere Text aus deu_mixed-typical_2011_10K-sentences.txt...\n",
            "Textextraktion abgeschlossen.\n",
            "Generiere N-Gramm-Modell...\n",
            "Modellgenerierung abgeschlossen.\n",
            "\n",
            "Auto-Complete Beispiel:\n",
            "Prompt: 'Er wurde im'\n",
            "Vorschläge: ['umkreis', 'schuljahr', '19.', 'prozeß', 'jahre']\n",
            "\n",
            "Prompt: 'Es gibt sie'\n",
            "Vorschläge: ['an', 'in']\n",
            "\n",
            "Prompt: 'Dieses Wort kommt nicht'\n",
            "Vorschläge: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_md\n",
        "nlp = spacy.load(\"de_core_news_md\")  # Kleines deutsches Modell\n",
        "# NER-Beispiel mit SpaCy\n",
        "print(\"\\nNER-Beispiel mit SpaCy:\")\n",
        "# Wähle einen kurzen Textausschnitt aus dem heruntergeladenen Corpus\n",
        "sample_text = full_text[:500] # Die ersten 500 Zeichen als Beispiel\n",
        "\n",
        "# Verarbeite den Beispieltext mit SpaCy\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Extrahiere benannte Entitäten (Named Entities)\n",
        "print(\"Benannte Entitäten im Beispieltext:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"  Text: {ent.text}, Typ: {ent.label_}, Erklärung: {spacy.explain(ent.label_)}\")\n",
        "\n",
        "print(\"\\nNER-Beispiel mit SpaCy:\")\n",
        "# Wähle einen kurzen Textausschnitt aus dem heruntergeladenen Corpus\n",
        "sample_text = \"Schrödinger kann nicht nur Programmieren. Er kennt sich jetzt auch mit dem Thema KI gut aus.\"\n",
        "\n",
        "# Verarbeite den Beispieltext mit SpaCy\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Extrahiere benannte Entitäten (Named Entities)\n",
        "print(\"Benannte Entitäten im Beispieltext:\")\n",
        "for ent in doc.ents:\n",
        "  print(f\"  Text: {ent.text}, Typ: {ent.label_}, Erklärung: {spacy.explain(ent.label_)}\")\n",
        "else:\n",
        "  print(\"Kein Text für das NER-Beispiel gefunden.\")\n"
      ],
      "metadata": {
        "id": "-eibPFopFrke",
        "outputId": "13bb26ec-4385-402b-a386-e70884f752d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.8.0/de_core_news_md-3.8.0-py3-none-any.whl (44.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-md\n",
            "Successfully installed de-core-news-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "NER-Beispiel mit SpaCy:\n",
            "Benannte Entitäten im Beispieltext:\n",
            "  Text: Zentralturm, Typ: LOC, Erklärung: Non-GPE locations, mountain ranges, bodies of water\n",
            "  Text: München, Typ: LOC, Erklärung: Non-GPE locations, mountain ranges, bodies of water\n",
            "  Text: Mariazell, Typ: LOC, Erklärung: Non-GPE locations, mountain ranges, bodies of water\n",
            "  Text: Deštnice, Typ: LOC, Erklärung: Non-GPE locations, mountain ranges, bodies of water\n",
            "  Text: SIRA-Bataillon, Typ: MISC, Erklärung: Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
            "  Text: Hannover, Typ: LOC, Erklärung: Non-GPE locations, mountain ranges, bodies of water\n",
            "  Text: Manfred-Wörner-Medaille, Typ: MISC, Erklärung: Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
            "\n",
            "NER-Beispiel mit SpaCy:\n",
            "Benannte Entitäten im Beispieltext:\n",
            "  Text: Schrödinger, Typ: PER, Erklärung: Named person or family.\n",
            "  Text: KI, Typ: MISC, Erklärung: Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
            "Kein Text für das NER-Beispiel gefunden.\n"
          ]
        }
      ]
    }
  ]
}